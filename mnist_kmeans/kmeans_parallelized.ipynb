{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf1036",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:31:22.235821Z",
     "start_time": "2023-12-19T17:31:21.522758Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import findspark\n",
    "import random as rd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e044460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:31:26.144225Z",
     "start_time": "2023-12-19T15:31:26.043876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759986c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:31:28.801503Z",
     "start_time": "2023-12-19T17:31:28.796922Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Receives a String filename with name of the csv file of the dataset.\n",
    "\n",
    "Returns: a RDD with the loaded data\n",
    "'''\n",
    "def parallelReadFile(filename):\n",
    "    # Loads the file\n",
    "    rdd_lines = sc.textFile(filename)\n",
    "\n",
    "    # Deletes the first row containing labels\n",
    "    header = rdd_lines.first() #or .take(1)\n",
    "    #data_lines = rdd_lines.filter(lambda line: line != header)\n",
    "    data_lines = rdd_lines.flatMap(lambda line: [line] if line != header else [])\n",
    "\n",
    "    # Divide each line into a list of individual values, then convert to a numpy array\n",
    "    def parse_line(line):\n",
    "        # Convert values to float\n",
    "        return np.array([float(v) for v in line.split(',')[1:]])  # Exclude first column\n",
    "\n",
    "    # Map each row to a numpy array\n",
    "    parsed_data = data_lines.map(lambda x: parse_line(x))\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739bebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:31:31.545628Z",
     "start_time": "2023-12-19T17:31:31.541712Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Receives a list of d-dimensional tuples called centroids, representing the current state\n",
    "of the centroids, and a d-dimensional tuple x which represents the datum to be assigned to a cluster.\n",
    "\n",
    "Returns: an integer with the index in centroids of the closest centroid to x\n",
    "'''\n",
    "def parallelAssign2cluster(x, centroids):\n",
    "    \n",
    "    # Initialisation of the distances from x to the centroids\n",
    "    distances = [0 for i in range(len(centroids))]\n",
    "    \n",
    "    # For each centroid, we compute the euclidian distance to x\n",
    "    for c in range(len(centroids)):\n",
    "        for i in range(784):\n",
    "            distances[c] += (x[i] - centroids[c][i]) ** 2\n",
    "        #distances[c] **= 0.5\n",
    "        distances[c] = np.sqrt(distances[c])\n",
    "    \n",
    "    # We look for the smallest distance which corresponds to the nearest centroid\n",
    "    dmin = distances[0]\n",
    "    for d in distances[1:]:\n",
    "        if d < dmin:\n",
    "            dmin = d\n",
    "            \n",
    "    return distances.index(dmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344e28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:31:35.505201Z",
     "start_time": "2023-12-19T17:31:35.500248Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Performs the serialized K-Means algorithm on the dataset X, grouping the instances into K different\n",
    "clusters. The number of iterations of the method to be executed is n_iter. The initialization of the centroids will be\n",
    "random, sampled from a standard normal distribution. It returns a list of length K with the d-dimensional centroids\n",
    "computed\n",
    "'''\n",
    "# Following the implementation described in the slides        \n",
    "def parallelKMeans(data, K, n_iter):\n",
    "    # Generate K random tuples with 28000 elements each (len of each x)\n",
    "    centroids = [tuple(np.random.randn(784)) for i in range(K)]\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        print(\"it : \", it)\n",
    "    \n",
    "        clustered_data = data.map(lambda x: (parallelAssign2cluster(tuple(x), centroids), np.append(x,1)))\n",
    "\n",
    "        new_clusters = clustered_data.reduceByKey(lambda x, y: x+y)\n",
    "        \n",
    "        def divide(row):\n",
    "            _, x = row\n",
    "            x = list(x)\n",
    "            for k in range(784):\n",
    "                x[k] = x[k] / x[-1]\n",
    "            x = tuple(x[:-1])\n",
    "            return x\n",
    "                \n",
    "        centroids = new_clusters.map(lambda row : divide(row)).collect()\n",
    "        plt.figure(figsize = (20,5))\n",
    "        i=1\n",
    "        for c in centroids:\n",
    "            ax = plt.subplot(1, K, i)\n",
    "            c = np.asarray(c)\n",
    "            plt.imshow(c.reshape(28,28),cmap='gray')\n",
    "            ax.set_axis_off() \n",
    "            i+=1\n",
    "        plt.show()\n",
    "        \n",
    "    return centroids # Only centroid for final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7f356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:31:39.781326Z",
     "start_time": "2023-12-19T17:31:39.778485Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(filename=\"tot_mnist_shuf.csv\", K=10):\n",
    "    \n",
    "    data = parallelReadFile(filename)\n",
    "    centroids = parallelKMeans(data, K, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90710685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:52:59.477855Z",
     "start_time": "2023-12-19T15:34:14.879253Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\", \"KMeansParallel\")\n",
    "\n",
    "start_time = time.time()\n",
    "main()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution time: \", end_time - start_time)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912907fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T21:35:23.467649Z",
     "start_time": "2023-12-19T20:09:20.536398Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cores = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "execution_times = []\n",
    "K = 10\n",
    "\n",
    "for cores in num_cores:\n",
    "    sc = SparkContext(master=f\"local[{cores}]\", appName=\"Kmeans PerformanceTest\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Running\n",
    "    main(\"tot_mnist_shuf.csv\", K)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_times.append(end_time - start_time)\n",
    "    sc.stop()\n",
    "    \n",
    "# Compute speedup\n",
    "base_time = execution_times[0]  # ExecTime with 1 worker\n",
    "speedups = [base_time / time for time in execution_times]\n",
    "\n",
    "print(speedups)\n",
    "print(execution_times)\n",
    "\n",
    "# Ploting Performance curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_cores, execution_times, marker='o')\n",
    "plt.title(\"Performance curve for the number of worker\")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"Execution time (seconds)\")\n",
    "\n",
    "# Ploting Speedup curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_cores, speedups, marker='o', color='green')\n",
    "plt.title(\"Speedup curve for the number of worker\")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"Time with one worker/time with n workers\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4212e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T20:09:20.531000Z",
     "start_time": "2023-12-19T19:04:26.626171Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cluster = [1, 3, 5, 7, 8, 9, 10, 11]\n",
    "execution_times = []\n",
    "sc = SparkContext(master=f\"local[*]\", appName=\"Kmeans PerformanceTest\")\n",
    "\n",
    "for K in num_cluster:\n",
    "    start_time = time.time()\n",
    "    # Running\n",
    "    main(\"tot_mnist_shuf.csv\", K)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_times.append(end_time - start_time)\n",
    "    \n",
    "# Compute speedup\n",
    "base_time = execution_times[0]  # ExecTime with 1 cluster\n",
    "speedups = [base_time / time for time in execution_times[1:]]\n",
    "\n",
    "# Ploting Speedup curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_cluster[1:], speedups, marker='o', color='green')\n",
    "plt.title(\"Speedup curve for K clusters\")\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"Time with one cluster/time with K cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
